{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä DriftWatch + MLflow ‚Äî Drift Tracking Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/VincentCotella/DriftWatch/blob/main/examples/notebooks/mlflow_tracking.ipynb)\n",
    "\n",
    "This notebook demonstrates how to **log drift detection results to MLflow** for experiment tracking.\n",
    "\n",
    "You'll learn:\n",
    "\n",
    "1. **Basic logging** ‚Äî Log a drift report to MLflow in 3 lines\n",
    "2. **Exploring results** ‚Äî View metrics, params, and artifacts in the MLflow UI\n",
    "3. **Multiple checks** ‚Äî Track drift over time across multiple runs\n",
    "4. **Pipeline integration** ‚Äî Log drift alongside your training metrics\n",
    "\n",
    "> **Requires:** `pip install driftwatch[mlflow]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DriftWatch with MLflow support\n",
    "!pip install -q driftwatch[mlflow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup ‚Äî Generate Sample Data\n",
    "\n",
    "We'll simulate reference (training) and production data with known drift patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Reference data (Training distribution)\n",
    "reference_df = pd.DataFrame(\n",
    "    {\n",
    "        \"age\": np.random.normal(30, 5, 1000),\n",
    "        \"income\": np.random.normal(50000, 10000, 1000),\n",
    "        \"credit_score\": np.random.normal(700, 50, 1000),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Production data WITH DRIFT on \"age\" only\n",
    "production_df = pd.DataFrame(\n",
    "    {\n",
    "        \"age\": np.random.normal(45, 5, 1000),  # DRIFT: Mean 30 -> 45\n",
    "        \"income\": np.random.normal(50000, 10000, 1000),  # No drift\n",
    "        \"credit_score\": np.random.normal(700, 50, 1000),  # No drift\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Reference Data:\")\n",
    "print(reference_df.describe().round(2))\n",
    "print(\"\\nProduction Data:\")\n",
    "print(production_df.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Detect Drift\n",
    "\n",
    "Standard DriftWatch workflow ‚Äî create a `Monitor` and check for drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from driftwatch import Monitor\n",
    "\n",
    "monitor = Monitor(\n",
    "    reference_data=reference_df,\n",
    "    features=[\"age\", \"income\", \"credit_score\"],\n",
    ")\n",
    "\n",
    "report = monitor.check(production_df)\n",
    "\n",
    "print(report.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Log to MLflow ‚Äî Basic Usage\n",
    "\n",
    "Now the exciting part: **log the drift report to MLflow** with just 3 lines!\n",
    "\n",
    "MLflow will automatically create a local `mlruns/` directory to store everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from driftwatch.integrations.mlflow import MLflowDriftTracker\n",
    "\n",
    "# Create a tracker pointing to an MLflow experiment\n",
    "tracker = MLflowDriftTracker(\n",
    "    experiment_name=\"drift-monitoring-demo\",\n",
    ")\n",
    "\n",
    "# Log the report -> creates an MLflow run\n",
    "run_id = tracker.log_report(report)\n",
    "\n",
    "print(\"Logged to MLflow!\")\n",
    "print(f\"   Run ID: {run_id}\")\n",
    "print(f\"   Experiment: {tracker.experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Inspect What Was Logged\n",
    "\n",
    "Let's use the MLflow Python client to see exactly what was recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Fetch the run we just created\n",
    "run = mlflow.get_run(run_id)\n",
    "\n",
    "print(\"TAGS\")\n",
    "print(\"-\" * 40)\n",
    "for key, value in sorted(run.data.tags.items()):\n",
    "    if key.startswith(\"driftwatch\"):\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nPARAMS\")\n",
    "print(\"-\" * 40)\n",
    "for key, value in sorted(run.data.params.items()):\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nMETRICS\")\n",
    "print(\"-\" * 40)\n",
    "for key, value in sorted(run.data.metrics.items()):\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the JSON Artifact\n",
    "\n",
    "The full drift report is also saved as a JSON artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Download the artifact\n",
    "artifact_path = mlflow.artifacts.download_artifacts(\n",
    "    run_id=run_id, artifact_path=\"driftwatch/drift_report.json\"\n",
    ")\n",
    "\n",
    "report_json = json.loads(Path(artifact_path).read_text())\n",
    "print(json.dumps(report_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Track Drift Over Time\n",
    "\n",
    "In production, you'd run drift checks periodically (daily, hourly, etc.).\n",
    "\n",
    "Let's simulate **3 days** of drift checks where drift gradually increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate 3 production batches with increasing drift\n",
    "drift_scenarios = [\n",
    "    {\"label\": \"Day 1 - Slight drift\", \"age_mean\": 32, \"age_std\": 5},\n",
    "    {\"label\": \"Day 2 - Moderate drift\", \"age_mean\": 38, \"age_std\": 6},\n",
    "    {\"label\": \"Day 3 - Severe drift\", \"age_mean\": 50, \"age_std\": 8},\n",
    "]\n",
    "\n",
    "tracker = MLflowDriftTracker(\n",
    "    experiment_name=\"drift-over-time\",\n",
    "    tags={\"env\": \"production\", \"model\": \"credit-risk-v2\"},\n",
    ")\n",
    "\n",
    "for scenario in drift_scenarios:\n",
    "    # Generate production data for this \"day\"\n",
    "    prod = pd.DataFrame(\n",
    "        {\n",
    "            \"age\": np.random.normal(scenario[\"age_mean\"], scenario[\"age_std\"], 500),\n",
    "            \"income\": np.random.normal(50000, 10000, 500),\n",
    "            \"credit_score\": np.random.normal(700, 50, 500),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Detect drift\n",
    "    report = monitor.check(prod)\n",
    "\n",
    "    # Log to MLflow with a descriptive run name\n",
    "    run_id = tracker.log_report(\n",
    "        report,\n",
    "        run_name=scenario[\"label\"],\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"{scenario['label']}: \"\n",
    "        f\"status={report.status.value}, \"\n",
    "        f\"drift_ratio={report.drift_ratio():.0%}, \"\n",
    "        f\"run_id={run_id[:8]}...\"\n",
    "    )\n",
    "\n",
    "print(\"\\nAll 3 runs logged to experiment 'drift-over-time'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Runs\n",
    "\n",
    "Use `mlflow.search_runs()` to compare drift metrics across all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all runs from the experiment\n",
    "experiment = mlflow.get_experiment_by_name(\"drift-over-time\")\n",
    "runs_df = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"start_time ASC\"],\n",
    ")\n",
    "\n",
    "# Show key metrics side by side\n",
    "comparison = runs_df[\n",
    "    [\n",
    "        \"run_id\",\n",
    "        \"tags.mlflow.runName\",\n",
    "        \"params.drift.status\",\n",
    "        \"metrics.drift.drift_ratio\",\n",
    "        \"metrics.drift.age.score\",\n",
    "        \"metrics.drift.num_drifted\",\n",
    "    ]\n",
    "].rename(\n",
    "    columns={\n",
    "        \"tags.mlflow.runName\": \"Run Name\",\n",
    "        \"params.drift.status\": \"Status\",\n",
    "        \"metrics.drift.drift_ratio\": \"Drift Ratio\",\n",
    "        \"metrics.drift.age.score\": \"Age Score\",\n",
    "        \"metrics.drift.num_drifted\": \"Drifted Features\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Drift Evolution Over Time\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Pipeline Integration\n",
    "\n",
    "In a real ML pipeline, you often want to log drift metrics **inside the same run** as your training metrics.\n",
    "\n",
    "Use `run_id` to attach drift data to an existing run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Simulate a training pipeline\n",
    "mlflow.set_experiment(\"training-pipeline\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"model-training-v3\") as run:\n",
    "    # ---- Step 1: Train model (simulated) ----\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_metric(\"accuracy\", 0.92)\n",
    "    mlflow.log_metric(\"f1_score\", 0.89)\n",
    "    print(\"Model trained (accuracy=0.92, f1=0.89)\")\n",
    "\n",
    "    # ---- Step 2: Check drift on new data ----\n",
    "    report = monitor.check(production_df)\n",
    "    print(f\"Drift check: status={report.status.value}\")\n",
    "\n",
    "    # ---- Step 3: Log drift INTO the same run ----\n",
    "    tracker = MLflowDriftTracker(experiment_name=\"training-pipeline\")\n",
    "    tracker.log_report(report, run_id=run.info.run_id)\n",
    "    print(f\"Drift logged to same run: {run.info.run_id[:8]}...\")\n",
    "\n",
    "# Now this single run contains BOTH training metrics AND drift metrics!\n",
    "final_run = mlflow.get_run(run.info.run_id)\n",
    "print(\"\\nAll metrics in this run:\")\n",
    "for key, value in sorted(final_run.data.metrics.items()):\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Custom Configuration\n",
    "\n",
    "### Custom Metric Prefix\n",
    "\n",
    "Use a custom prefix to namespace metrics ‚Äî useful when tracking multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track drift for two different models in the same experiment\n",
    "tracker_v1 = MLflowDriftTracker(\n",
    "    experiment_name=\"multi-model-drift\",\n",
    "    prefix=\"model_v1\",  # Metrics: model_v1.has_drift, model_v1.age.score, etc.\n",
    ")\n",
    "\n",
    "tracker_v2 = MLflowDriftTracker(\n",
    "    experiment_name=\"multi-model-drift\",\n",
    "    prefix=\"model_v2\",  # Metrics: model_v2.has_drift, model_v2.age.score, etc.\n",
    ")\n",
    "\n",
    "run_id_v1 = tracker_v1.log_report(report, run_name=\"v1-check\")\n",
    "run_id_v2 = tracker_v2.log_report(report, run_name=\"v2-check\")\n",
    "\n",
    "print(f\"Model v1 logged (prefix=model_v1): {run_id_v1[:8]}...\")\n",
    "print(f\"Model v2 logged (prefix=model_v2): {run_id_v2[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable Artifact Logging\n",
    "\n",
    "If you only want metrics (no JSON file), disable artifact logging to save storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_light = MLflowDriftTracker(\n",
    "    experiment_name=\"lightweight-tracking\",\n",
    "    log_report_artifact=False,  # No JSON artifact\n",
    ")\n",
    "\n",
    "run_id = tracker_light.log_report(report)\n",
    "print(f\"Metrics only (no artifact): {run_id[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the MLflow UI\n",
    "\n",
    "To visualize all of this in the MLflow dashboard, run this in your terminal:\n",
    "\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "```\n",
    "\n",
    "Then open [http://localhost:5000](http://localhost:5000) in your browser.\n",
    "\n",
    "You'll see:\n",
    "- Drift metrics plotted over time\n",
    "- Per-feature scores for each run\n",
    "- Parameters showing reference/production sizes\n",
    "- Artifacts with the full JSON report\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Remove the local `mlruns/` directory created by this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "mlruns_path = Path(\"mlruns\")\n",
    "if mlruns_path.exists():\n",
    "    shutil.rmtree(mlruns_path)\n",
    "    print(\"Cleaned up mlruns/ directory\")\n",
    "else:\n",
    "    print(\"No mlruns/ directory to clean up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "| Feature | Code |\n",
    "|---------|------|\n",
    "| **Basic logging** | `tracker.log_report(report)` |\n",
    "| **Named runs** | `tracker.log_report(report, run_name=\"day-1\")` |\n",
    "| **Existing run** | `tracker.log_report(report, run_id=run_id)` |\n",
    "| **Custom prefix** | `MLflowDriftTracker(prefix=\"model_v2\")` |\n",
    "| **Disable artifacts** | `MLflowDriftTracker(log_report_artifact=False)` |\n",
    "| **Custom tags** | `MLflowDriftTracker(tags={\"env\": \"prod\"})` |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- [Full Documentation](https://vincentcotella.github.io/DriftWatch/)\n",
    "- [Drift Detection Tutorial](./drift_detection_tutorial.ipynb)\n",
    "- [Slack Alerting](https://vincentcotella.github.io/DriftWatch/integrations/slack/)\n",
    "- [FastAPI Integration](https://vincentcotella.github.io/DriftWatch/integrations/fastapi/)\n",
    "\n",
    "---\n",
    "\n",
    "If you found DriftWatch useful, please star us on [GitHub](https://github.com/VincentCotella/DriftWatch)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}